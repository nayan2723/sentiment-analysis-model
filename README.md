# Sentiment Analysis with Transformers ğŸ§ ğŸ’¬

A simple, end-to-end **Sentiment Analysis** project built using **PyTorch**, **HuggingFace Transformers**, and the **Datasets** library.
This project trains a text classification model on custom data and evaluates its performance, with support for inference using HuggingFace pipelines.

---

## ğŸš€ Features

* Load and preprocess datasets using ğŸ¤— *datasets*
* Tokenize text using pretrained Transformer tokenizers
* Fine-tune a state-of-the-art model (BERT/RoBERTa/DistilBERT, etc.)
* Train using HuggingFace `Trainer`
* Evaluate and visualize performance
* Run predictions using a simple inference wrapper

---

## ğŸ› ï¸ Tech Stack

* **Python 3**
* **PyTorch**
* **Transformers (HuggingFace)**
* **Datasets (HuggingFace)**
* **scikit-learn**
* **pandas, numpy**

---

## ğŸ“‚ Project Structure

```
Sentiment_Analysis/
â”‚
â”œâ”€â”€ notebook.ipynb      # Main training notebook
â”œâ”€â”€ data/               # (Optional) Training dataset
â”œâ”€â”€ models/             # Saved models/checkpoints
â”œâ”€â”€ README.md           # Project documentation
â””â”€â”€ requirements.txt    # Dependency list
```

---

## ğŸ”§ Setup Instructions

### 1ï¸âƒ£ Install Dependencies

```bash
pip install torch transformers datasets scikit-learn pandas numpy
```

### 2ï¸âƒ£ Verify GPU Availability

```python
import torch
print(torch.cuda.is_available())
```

---

## ğŸ‹ï¸ Training the Model

### âœ” Load and split dataset

### âœ” Tokenize text

```python
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

### âœ” Load model

```python
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=2
)
```

### âœ” Define training configuration

```python
training_args = TrainingArguments(
    output_dir="model_output",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3
)
```

### âœ” Train using Trainer

```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer
)

trainer.train()
```

---

## ğŸ“Š Evaluation

```python
metrics = trainer.evaluate()
print(metrics)
```

Outputs include:

* Accuracy
* Precision / Recall / F1
* Loss curves

---

## ğŸ”® Running Inference

```python
from transformers import pipeline

pipe = pipeline("sentiment-analysis", model="model_output")

pipe("This product is amazing!")
```

---

## ğŸ“¦ Model Export

Fine-tuned model is saved automatically under:

```
model_output/
```

---

## â¤ï¸ Author

**Nayan Kshitij**
Cybersecurity & AI Enthusiast
